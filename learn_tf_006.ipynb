{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM+3DOompSfjcVPEEtQtsSA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bikash119/learn_tensorflow/blob/main/learn_tf_006.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLP: SkimLit ðŸ”¥\n",
        "### We will be replicating the deep learning model behind the 2017 paper [PubMed: A Dataset for Sequential Sentence Classification in Medical Abstracts](https://arxiv.org/abs/1710.06071)\n",
        "\n",
        "**The Problem:** The number of RCT papers releases is continuing to increase, those without structured abstracts can be hard to read and in turn slow down researchers moving through the literature\n",
        "\n",
        "**The Solution:** Create a NLP model to classify abstract sentences into the role they play from the list of possible roles (objective, method, result, background or conclusion)\n",
        "\n",
        "The data comes from: [PubMed: A Dataset for Sequential Sentence Classification in Medical Abstracts](https://arxiv.org/abs/1710.06071)\n",
        "\n",
        "The model is from: [Neural Networks for Joint Sentence Classification\n",
        "in Medical Paper Abstracts](https://arxiv.org/pdf/1612.05251.pdf)\n"
      ],
      "metadata": {
        "id": "64IFcTvv0Yc5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Steps to follow\n",
        "1. Download the data\n",
        "2. Perform Pre-processing\n",
        "3. Create a baseline model"
      ],
      "metadata": {
        "id": "rV6iDFme6GCv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Dowload the data"
      ],
      "metadata": {
        "id": "x27pmFx36Vgb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1QoY6-r0T-X",
        "outputId": "d5fa5ad9-0fb2-4185-d572-3d5ed8dfd45b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'pubmed-rct' already exists and is not an empty directory.\n",
            "PubMed_200k_RCT\n",
            "PubMed_200k_RCT_numbers_replaced_with_at_sign\n",
            "PubMed_20k_RCT\n",
            "PubMed_20k_RCT_numbers_replaced_with_at_sign\n",
            "README.md\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Franck-Dernoncourt/pubmed-rct.git\n",
        "!ls pubmed-rct"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the dataset `PubMed_20k_RCT_numbers_replaced_with_at_sign` for faster iterations on smaller data size. The `PubMed_200k_RCT_numbers_replaced_with_at_sign` is superset of `PubMed_20k_RCT_numbers_replaced_with_at_sign` where all the numbers (numerical values) are replaced with `at-sign`"
      ],
      "metadata": {
        "id": "gVZ0G55g6r8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls pubmed-rct/PubMed_200k_RCT_numbers_replaced_with_at_sign"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5H8v2a866fbp",
        "outputId": "ca54218f-8d9e-4472-a6b7-eb76335eb925"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dev.txt  test.txt  train.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**dev.txt**: Validation dataset\n",
        "\n",
        "**train.txt**: Training dataset\n",
        "\n",
        "**test.txt**: Test dataset"
      ],
      "metadata": {
        "id": "URQXbh3g7qye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Lets turn the filepath and the filename to a variable\n",
        "data_dir = \"/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/\"\n",
        "# List all the files in a directory\n",
        "filenames= [data_dir+filename for filename in os.listdir(data_dir)]\n",
        "filenames"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fu98tKM_7i4r",
        "outputId": "5e7112c5-9d43-452d-ca36-8df66ac00d8c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/dev.txt',\n",
              " '/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/train.txt',\n",
              " '/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/test.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Visualize, Visualize, Visualize\n",
        "```"
      ],
      "metadata": {
        "id": "m7-7pLyQ9sVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A function to read lines of a target file\n",
        "from typing import List\n",
        "\n",
        "def read_lines(filename) -> List[str]:\n",
        "  \"\"\"\n",
        "    Reads a file and returns the lines of the file as list\n",
        "    Args:\n",
        "      filename(str): Name of the file\n",
        "    Returns(List[str]):\n",
        "      A list containing the lines in the file\n",
        "  \"\"\"\n",
        "  with open(filename,\"r\") as f:\n",
        "    return f.readlines()"
      ],
      "metadata": {
        "id": "SK8Vo1M-8kaL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_text = read_lines(data_dir+\"train.txt\")\n",
        "train_text[:20]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmMvn7-1-tE6",
        "outputId": "8e8803c1-a927-4c58-e086-0c1a056e3963"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['###24293578\\n',\n",
              " 'OBJECTIVE\\tTo investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( OA ) .\\n',\n",
              " 'METHODS\\tA total of @ patients with primary knee OA were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .\\n',\n",
              " 'METHODS\\tOutcome measures included pain reduction and improvement in function scores and systemic inflammation markers .\\n',\n",
              " 'METHODS\\tPain was assessed using the visual analog pain scale ( @-@ mm ) .\\n',\n",
              " 'METHODS\\tSecondary outcome measures included the Western Ontario and McMaster Universities Osteoarthritis Index scores , patient global assessment ( PGA ) of the severity of knee OA , and @-min walk distance ( @MWD ) .\\n',\n",
              " 'METHODS\\tSerum levels of interleukin @ ( IL-@ ) , IL-@ , tumor necrosis factor ( TNF ) - , and high-sensitivity C-reactive protein ( hsCRP ) were measured .\\n',\n",
              " 'RESULTS\\tThere was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , PGA , and @MWD at @ weeks .\\n',\n",
              " 'RESULTS\\tThe mean difference between treatment arms ( @ % CI ) was @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; and @ ( @-@ @ ) , p < @ , respectively .\\n',\n",
              " 'RESULTS\\tFurther , there was a clinically relevant reduction in the serum levels of IL-@ , IL-@ , TNF - , and hsCRP at @ weeks in the intervention group when compared to the placebo group .\\n',\n",
              " 'RESULTS\\tThese differences remained significant at @ weeks .\\n',\n",
              " 'RESULTS\\tThe Outcome Measures in Rheumatology Clinical Trials-Osteoarthritis Research Society International responder rate was @ % in the intervention group and @ % in the placebo group ( p < @ ) .\\n',\n",
              " 'CONCLUSIONS\\tLow-dose oral prednisolone had both a short-term and a longer sustained effect resulting in less knee pain , better physical function , and attenuation of systemic inflammation in older patients with knee OA ( ClinicalTrials.gov identifier NCT@ ) .\\n',\n",
              " '\\n',\n",
              " '###24854809\\n',\n",
              " 'BACKGROUND\\tEmotional eating is associated with overeating and the development of obesity .\\n',\n",
              " 'BACKGROUND\\tYet , empirical evidence for individual ( trait ) differences in emotional eating and cognitive mechanisms that contribute to eating during sad mood remain equivocal .\\n',\n",
              " 'OBJECTIVE\\tThe aim of this study was to test if attention bias for food moderates the effect of self-reported emotional eating during sad mood ( vs neutral mood ) on actual food intake .\\n',\n",
              " 'OBJECTIVE\\tIt was expected that emotional eating is predictive of elevated attention for food and higher food intake after an experimentally induced sad mood and that attentional maintenance on food predicts food intake during a sad versus a neutral mood .\\n',\n",
              " 'METHODS\\tParticipants ( N = @ ) were randomly assigned to one of the two experimental mood induction conditions ( sad/neutral ) .\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations from reading the content of `train.txt`\n",
        "1. Each sentence is prefixed with _role_ and separated by (`\\t`) from the content of the sentence.\n",
        "2. Each sentence ends with (`\\n`) and a comma (`,`)\n",
        "3. Every abstract is separated by abstract Id which starts with three hash (`###`) followed by a number and ends with (`\\n`) and comma (`,`)\n",
        "4. Every abstract ends with (`\\n`) in a line\n",
        "\n",
        "We will have to write a function such that all the lines in the target file are returned as list of dictionaries containing key/value pairs:\n",
        "  \n",
        "  * `\"line_number\"`: the position of the line in the abstract\n",
        "  * `\"target\"`: the role of the line in the abstract\n",
        "  * `\"text\"`: the text of the line in the abstract\n",
        "  * `\"total_lines\"`: the total lines in an abstract sample\n",
        "\n",
        "  Example output for a single line in an abstract\n",
        "  ```\n",
        "  [\n",
        "    {\n",
        "      \"line_number\":3\n",
        "      ,\"target\": \"OBJECTIVE\"\n",
        "      ,\"text\":\"It was expected that emotional eating is predictive of elevated attention for food and higher food intake after an experimentally induced sad mood and that attentional maintenance on food predicts food intake during a sad versus a neutral mood .\"\n",
        "      ,\"total_lines\": 11\n",
        "    }\n",
        "    ,{\n",
        "\n",
        "    }\n",
        "  ]  \n",
        "  ```"
      ],
      "metadata": {
        "id": "_vYnlrKOAEG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict,List\n",
        "def preprocess_text_with_line_numbers(filename) -> List[Dict]:\n",
        "  \"\"\"\n",
        "    Takes a filename and returns the contents of the file as list of dictionaries.\n",
        "    Args:\n",
        "      filename(str): The name of the target file\n",
        "    Returns:\n",
        "      abstract_lines(List[Dict]): A list of dictionaries each containing a line from the abstract.\n",
        "      Example output:\n",
        "        [\n",
        "          {\n",
        "            \"line_number\":3\n",
        "            ,\"target\": \"OBJECTIVE\"\n",
        "            ,\"text\":\"It was expected that emotional eating is predictive of elevated attention for food and higher food intake after an experimentally induced sad mood and that attentional maintenance on food predicts food intake during a sad versus a neutral mood .\"\n",
        "            ,\"total_lines\": 11\n",
        "          }\n",
        "          ,{\n",
        "            \"line_number\":4\n",
        "            ,\"target\": \"BACKGROUND\"\n",
        "            ,\"text\":\"Emotional eating is associated with overeating and the development of obesity .\"\n",
        "            ,\"total_lines\": 13\n",
        "          }\n",
        "        ]\n",
        "  \"\"\"\n",
        "  # all_abstracts = []\n",
        "  # abstract_line=\"\"\n",
        "  # file_content = read_lines(filename)\n",
        "  # line_number = 0\n",
        "  # abstract_line = {}\n",
        "  # for line in file_content:\n",
        "  #   # Check if the line contains abstract id\n",
        "  #   if line.startswith(\"###\"):\n",
        "  #     abstract_id = line\n",
        "  #   elif line in ['\\n']:\n",
        "  #     abstract_line[\"total_lines\"] = line_number-1\n",
        "  #     all_abstracts.append(abstract_line)\n",
        "  #     line_number = 0\n",
        "  #     abstract_line = {}\n",
        "  #   else:\n",
        "  #     target,text = line.split(\"\\t\")\n",
        "  #     line_number += 1\n",
        "  #     abstract_line[\"line_number\"] = line_number\n",
        "  #     abstract_line[\"target\"] = target\n",
        "  #     abstract_line[\"text\"] = text\n",
        "  # return all_abstracts\n",
        "  input_lines = read_lines(filename) # get all lines from filename\n",
        "  abstract_lines = \"\" # create an empty abstract\n",
        "  abstract_samples = [] # create an empty list of abstracts\n",
        "\n",
        "  # Loop through each line in target file\n",
        "  for line in input_lines:\n",
        "    if line.startswith(\"###\"): # check to see if line is an ID line\n",
        "      abstract_id = line\n",
        "      abstract_lines = \"\" # reset abstract string\n",
        "    elif line.isspace(): # check to see if line is a new line\n",
        "      abstract_line_split = abstract_lines.splitlines() # split abstract into separate lines\n",
        "\n",
        "      # Iterate through each line in abstract and count them at the same time\n",
        "      for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n",
        "        line_data = {} # create empty dict to store data from line\n",
        "        target_text_split = abstract_line.split(\"\\t\") # split target label from text\n",
        "        line_data[\"target\"] = target_text_split[0] # get target label\n",
        "        line_data[\"text\"] = target_text_split[1].lower() # get target text and lower it\n",
        "        line_data[\"line_number\"] = abstract_line_number # what number line does the line appear in the abstract?\n",
        "        line_data[\"total_lines\"] = len(abstract_line_split) - 1 # how many total lines are in the abstract? (start from 0)\n",
        "        abstract_samples.append(line_data) # add line data to abstract samples list\n",
        "\n",
        "    else: # if the above conditions aren't fulfilled, the line contains a labelled sentence\n",
        "      abstract_lines += line\n",
        "\n",
        "  return abstract_samples\n"
      ],
      "metadata": {
        "id": "zoJiw6oj-0iu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_abstracts=preprocess_text_with_line_numbers(data_dir+\"train.txt\")"
      ],
      "metadata": {
        "id": "Tb17edu9IslK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_abstracts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCDzxv_zIyPX",
        "outputId": "b9d79b81-6cdd-42bb-a044-b0a044ce30c5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "180040"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "train_samples = preprocess_text_with_line_numbers(data_dir+\"train.txt\")\n",
        "val_samples = preprocess_text_with_line_numbers(data_dir+\"dev.txt\")\n",
        "test_samples = preprocess_text_with_line_numbers(data_dir+\"test.txt\")\n",
        "len(train_samples), len(val_samples),len(test_samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DQS5kO2Q1mC",
        "outputId": "650adf6f-a42b-4c45-c9cd-73ebf5b05a37"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 638 ms, sys: 101 ms, total: 740 ms\n",
            "Wall time: 768 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(180040, 30212, 30135)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_df = pd.DataFrame(train_samples)\n",
        "val_df = pd.DataFrame(val_samples)\n",
        "test_df = pd.DataFrame(test_samples)"
      ],
      "metadata": {
        "id": "__VJ5DOaRm58"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_df),len(val_df), len(test_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eu3z2PIeVz3k",
        "outputId": "49ccdc88-c7ff-4fe1-e54b-c1f0b0e2be76"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(180040, 30212, 30135)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.target.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gj4QjSA4V_mJ",
        "outputId": "b0fad940-b221-467e-e1f1-7cfa61469852"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "METHODS        59353\n",
              "RESULTS        57953\n",
              "CONCLUSIONS    27168\n",
              "BACKGROUND     21727\n",
              "OBJECTIVE      13839\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.total_lines.plot.hist();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "6mq1rAWxWDjH",
        "outputId": "c4231ba8-9f6e-432c-daff-736a49bffa88"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences = train_df[\"text\"].tolist()\n",
        "val_sentences = val_df[\"text\"].tolist()\n",
        "test_sentences = test_df[\"text\"].tolist()\n",
        "len(train_sentences),len(val_sentences),len(test_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCQ48KVvWVA_",
        "outputId": "0012b896-76dc-48ff-e3fa-3d224b98b66d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(180040, 30212, 30135)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make numerical labels\n",
        "We're going to create one hot and label encoded labels\n",
        "> We could get away with just making label encoded labels, however, Tensorflow's CategoricalCrossEntropy loss function likes to have one hot encoded labels"
      ],
      "metadata": {
        "id": "hMQB8IE_T5KP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6QysElpYVrTF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One hot encoded labels\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "one_hot_encoder = OneHotEncoder(sparse=False)\n",
        "train_labels_one_hot = one_hot_encoder.fit_transform(train_df[\"target\"].to_numpy().reshape(-1, 1))\n",
        "val_labels_one_hot = one_hot_encoder.fit_transform(val_df[\"target\"].to_numpy().reshape(-1,1))\n",
        "test_labels_one_hot = one_hot_encoder.fit_transform(test_df[\"target\"].to_numpy().reshape(-1,1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6U4wepHBW6vd",
        "outputId": "07fdd469-263b-4c20-8cce-8a600d49da32"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels_one_hot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7r9N-OFXf_n",
        "outputId": "9d70ec76-6376-4310-c67b-5d0988fe3fb0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 1., 0.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., 0., 1.],\n",
              "       [0., 1., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Extract labels ( targets ) and encode them into integers\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels_encoded = label_encoder.fit_transform(train_df[\"target\"].to_numpy())\n",
        "val_labels_encoded = label_encoder.fit_transform(val_df[\"target\"].to_numpy())\n",
        "test_labels_encoded = label_encoder.fit_transform(test_df[\"target\"].to_numpy())"
      ],
      "metadata": {
        "id": "JL7yV6drW8Vg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels_encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cfc5L9dXTM8",
        "outputId": "3a2c0adc-b33e-442d-ae07-5adbb54880bb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 2, 2, ..., 4, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(label_encoder.classes_)\n",
        "classes = label_encoder.classes_\n",
        "num_classes,classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28VUWUZLYE9t",
        "outputId": "2cd3bbb4-2521-43f3-b749-477766975385"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5,\n",
              " array(['BACKGROUND', 'CONCLUSIONS', 'METHODS', 'OBJECTIVE', 'RESULTS'],\n",
              "       dtype=object))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 0: Getting a baseline\n",
        "Our first model will be a Tf-Idf Multinomial Naive Bayes as recommended by [Scikit-Learn's Machine Learning map](https://dev.mrdbourke.com/tensorflow-deep-learning/09_SkimLit_nlp_milestone_project_2/#model-0-getting-a-baseline)\n",
        "\n",
        "To build it we will use Scikit-Learn's `pipeline` which use `TfidfVectorizer` class to convert our abstract sentences into numbers using TF-IDF (term frequency- inverse document frequency) algorithm and then learn's to classify the sentence using `MultinomialNB` algorithm"
      ],
      "metadata": {
        "id": "q1U29qV1yCmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Create a pipeline\n",
        "model_0 = Pipeline([\n",
        "    (\"tf-idf\",TfidfVectorizer())\n",
        "    ,(\"clf\",MultinomialNB())\n",
        "])\n",
        "\n",
        "model_0.fit(X=train_sentences,\n",
        "            y=train_labels_encoded)"
      ],
      "metadata": {
        "id": "SHYXJTmhYSwT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "55f8f756-a624-43aa-9138-5abab2afef09"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('tf-idf', TfidfVectorizer()), ('clf', MultinomialNB())])"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tf-idf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tf-idf&#x27;, TfidfVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.score(X=val_sentences\n",
        "              ,y=val_labels_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdxAiv-D0IDB",
        "outputId": "5b0d9e38-141a-4ffe-a1c1-b129c4fc133c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7218323844829869"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_preds = model_0.predict(val_sentences)\n",
        "baseline_preds"
      ],
      "metadata": {
        "id": "ZYhMlEtm0M4J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84981b6d-303e-455b-9d84-4f12a8a6f5d4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4, 1, 3, ..., 4, 4, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to define a function to calculate the following model metrics\n",
        "1. Accuracy\n",
        "2. Precision\n",
        "3. Recall\n",
        "4. f1-score"
      ],
      "metadata": {
        "id": "v77J1ye67x3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from typing import Dict\n",
        "def calculate_results(y_true,y_pred)-> Dict:\n",
        "  \"\"\"\n",
        "    Calculate the model evaluation metrics, namely : accuracy, precision, recall, fscore\n",
        "    Args:\n",
        "      y_true(List[Any]): True labels\n",
        "      y_pred(List[Any]): Predicted labels\n",
        "    Returns:\n",
        "      model_results(Dict)\n",
        "  \"\"\"\n",
        "  accuracy = accuracy_score(y_true=y_true,y_pred=y_pred) * 100\n",
        "  # calculate the precision, recall and f1_score using the \"weighted\" average\n",
        "  precision,recall,f1_score,_ = precision_recall_fscore_support(y_true=y_true,y_pred=y_pred,average=\"weighted\")\n",
        "  model_results = {\n",
        "      \"accuracy\":accuracy\n",
        "      ,\"precision\":precision\n",
        "      ,\"recall\":recall\n",
        "      ,\"f1score\":f1_score\n",
        "  }\n",
        "  return model_results\n"
      ],
      "metadata": {
        "id": "7ahgLDKQ7pxs"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_results = calculate_results(y_true=val_labels_encoded,y_pred=baseline_preds)\n",
        "baseline_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2AFBPf_9uuJ",
        "outputId": "514f1250-03e1-4c67-bd11-ac262c9796d3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 72.1832384482987,\n",
              " 'precision': 0.7186466952323352,\n",
              " 'recall': 0.7218323844829869,\n",
              " 'f1score': 0.6989250353450294}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing the data from deep sequence models\n",
        "Before we embark on the journey of creating model, we will have to _numericalize_ our data using text vectorizer and embedding layers. Hence, we will have to create our own **text vectorizer** and **embedding layer**"
      ],
      "metadata": {
        "id": "MIbbErnL_Qge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "tzkzHQSE97M4"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since, we will be dealing with text, its a good practice to find out the length of each line to be used for training.\n",
        "> _In general, neural networks works best when the inputs are of the same size. This is important as it will be used to create batches of same size\n",
        "For example, if one sentence is eight words long and another is 29 words long, we want to pad the eight word sentence with zeros so it ends up being the same length as 29 words sentence_."
      ],
      "metadata": {
        "id": "g9bDeUXsAeec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's find the average length of sentences in our training set\n",
        "sentence_lengths = [len(sentence.split()) for sentence in train_sentences ]\n",
        "avg_len = sum(sentence_lengths)/len(train_sentences)\n",
        "avg_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGoHwSPvAdqE",
        "outputId": "8f344266-868c-4cfc-83dd-2c2fd3d25ef1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26.338269273494777"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.hist(sentence_lengths,bins=7);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "QqCNBCdYCBx4",
        "outputId": "0ced3317-7a2a-4051-e5ef-368486b8ff19"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How long of a sentence covers 95% of the lengths?\n",
        "output_seq_len = int(np.percentile(sentence_lengths,95))\n",
        "output_seq_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSLlmw2kDHYj",
        "outputId": "3aaaf271-5a2d-4137-9b04-d797263052a4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **ðŸ”‘Note**: Good practices to be followed when working with text corpus for a NLP problem\n",
        "1. What is the length of longest text?\n",
        "2. What is the length of smallest text?\n",
        "3. What is the distribution of length of text?\n",
        "4. One-hot encode the target labels\n",
        "5. LabelEncode the target labels\n"
      ],
      "metadata": {
        "id": "GPTEcPxiW1TR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# As mentioned in Table.1\n",
        "max_tokens=68000\n",
        "#Create text vectorizer\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "text_vectorizer = TextVectorization(max_tokens=max_tokens\n",
        "                                 ,output_sequence_length=55) ## 55 since, we saw that 95% percent of lines in abstract are of length 55\n",
        "\n",
        "# adapt text vectorizer to training data\n",
        "\n",
        "text_vectorizer.adapt(train_sentences)\n"
      ],
      "metadata": {
        "id": "0Vt69OyvWAvV"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test our text vectorizer\n",
        "import random\n",
        "target_sentence = random.choice(train_sentences)\n",
        "print(f\"Text:\\n {target_sentence}\")\n",
        "print(f\"Length of text:\\n {len(target_sentence.split())}\")\n",
        "print(f\"Vectorized Text:\\n {text_vectorizer([target_sentence])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yW8HDV_haRjR",
        "outputId": "e8a22861-414f-4d27-b1b6-490db2ce4075"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text:\n",
            " gilead sciences .\n",
            "Length of text:\n",
            " 3\n",
            "Vectorized Text:\n",
            " [[13288  4687     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorizer.get_config()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7olFjbisc7GR",
        "outputId": "3d4a7970-c889-4847-c796-d5a0d8c5a955"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'text_vectorization',\n",
              " 'trainable': True,\n",
              " 'dtype': 'string',\n",
              " 'batch_input_shape': (None,),\n",
              " 'max_tokens': 68000,\n",
              " 'standardize': 'lower_and_strip_punctuation',\n",
              " 'split': 'whitespace',\n",
              " 'ngrams': None,\n",
              " 'output_mode': 'int',\n",
              " 'output_sequence_length': 55,\n",
              " 'pad_to_max_tokens': False,\n",
              " 'sparse': False,\n",
              " 'ragged': False,\n",
              " 'vocabulary': None,\n",
              " 'idf_weights': None,\n",
              " 'encoding': 'utf-8',\n",
              " 'vocabulary_size': 64841}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create custom text embedding\n",
        "Our `text_vectorization` layers maps the words in our text directly to numbers.However, this doesn't capture the context of the sentence or in other words relation between the numbers\n",
        "\n",
        "To create richer numerical representation of our text, we need to use **embedding**\n",
        "\n",
        "As our model learns (by going through many different examples of abstract sentences and their labels), it'll update its embedding to better represent the relationship between tokens (numbers) in our corpus, hence will be able to gain the context of the sentences.\n",
        "\n",
        "We can create trainable embeddings using Tensorflow's `Embedding` layer.\n",
        "\n",
        "Again, here we are concerned about the input and output parameters of our `Embedding` layer.\n",
        "\n",
        "The `input_dim` describes the vocabulary size (68K in our case). And the `output_dim` parameter defines the dimension of the embedding output.\n",
        "\n",
        "Once created, our embedding layer will take the output of `text_vectorization` as input and convert them into feature vector of size `output_dim`"
      ],
      "metadata": {
        "id": "8YfCF-kwhIC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create token embedding layer\n",
        "rct_20k_text_vocab = text_vectorizer.get_vocabulary()\n",
        "token_embed = layers.Embedding(input_dim=len(rct_20k_text_vocab)\n",
        "                               ,output_dim=128\n",
        "                               ,mask_zero=True\n",
        "                               ,name=\"model_0_embedding\"\n",
        "                               )\n",
        "\n",
        "print(f\" Sentence before vectorization:{target_sentence}\")\n",
        "vectorized_text = text_vectorizer([target_sentence])\n",
        "print(f\" Sentence after vectorization before embedding: {vectorized_text}\")\n",
        "trainable_embeddings = token_embed(vectorized_text)\n",
        "print(f\" Sentence after  embedding: {trainable_embeddings}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_NO8TkYeFus",
        "outputId": "c6ef6029-b032-4caf-a453-872c34430d84"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Sentence before vectorization:gilead sciences .\n",
            " Sentence after vectorization before embedding: [[13288  4687     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0]]\n",
            " Sentence after  embedding: [[[ 0.0225707   0.01315497  0.02290453 ... -0.00660989 -0.02013282\n",
            "   -0.02901326]\n",
            "  [ 0.04305532 -0.02737774 -0.02696563 ... -0.04063536  0.01337457\n",
            "    0.02795197]\n",
            "  [-0.03964336  0.02545181 -0.0464378  ... -0.02052912 -0.02334999\n",
            "   -0.0222226 ]\n",
            "  ...\n",
            "  [-0.03964336  0.02545181 -0.0464378  ... -0.02052912 -0.02334999\n",
            "   -0.0222226 ]\n",
            "  [-0.03964336  0.02545181 -0.0464378  ... -0.02052912 -0.02334999\n",
            "   -0.0222226 ]\n",
            "  [-0.03964336  0.02545181 -0.0464378  ... -0.02052912 -0.02334999\n",
            "   -0.0222226 ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "def create_tensorboard_callback(dir_name,experiment_name):\n",
        "  \"\"\"\n",
        "    This function is used as a callback during the training of a model.\n",
        "    This will store the training experiments as we iterate over various strategies of\n",
        "    building / training the model\n",
        "  \"\"\"\n",
        "  log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "  return tf.keras.callbacks.TensorBoard(log_dir=log_dir)"
      ],
      "metadata": {
        "id": "I0h_SWIC1k0R"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Datasets"
      ],
      "metadata": {
        "id": "59gBQhpi5adS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_sentences),len(val_sentences)"
      ],
      "metadata": {
        "id": "l-cVVsEs9fuc",
        "outputId": "7df46ee7-304f-46e7-e20a-e56b9ab4c8a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(180040, 30212)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset= tf.data.Dataset.from_tensor_slices((train_sentences,train_labels_one_hot))\n",
        "valid_dataset= tf.data.Dataset.from_tensor_slices((val_sentences,val_labels_one_hot))\n",
        "test_dataset= tf.data.Dataset.from_tensor_slices((test_sentences,test_labels_one_hot))\n",
        "train_dataset"
      ],
      "metadata": {
        "id": "n_lEhtW45fvQ",
        "outputId": "8ef62e90-c29d-46fe-e5d5-46e8aefab4c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(5,), dtype=tf.float64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "val_dataset = valid_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "train_dataset"
      ],
      "metadata": {
        "id": "fDW7aoJw6R_9",
        "outputId": "71e6e00d-9aa7-4f57-fda1-339edf6eb665",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 1: Conv1D with token embeddings\n",
        "All our deep learning models will have a similar structure\n",
        "```\n",
        "Text(input) -> Tokenize -> Embedding -> Layers -> Output (Label Probabilities)\n",
        "```"
      ],
      "metadata": {
        "id": "BBy1-xsbygaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a 1D convolutional model to process sentences\n",
        "inputs= layers.Input(shape=(1,),dtype=tf.string)\n",
        "text_vectors= text_vectorizer(inputs)\n",
        "token_embeddings= token_embed(text_vectors)\n",
        "print(f\"Shape of embedding tokens: {token_embeddings.shape}\")\n",
        "x= layers.Conv1D(64,kernel_size=5,padding=\"same\",activation=\"relu\")(token_embeddings)\n",
        "x= layers.GlobalAveragePooling1D()(x)\n",
        "outputs= layers.Dense(num_classes,activation=\"softmax\")(x)\n",
        "model_1= tf.keras.Model(inputs,outputs,name=\"conv1d\")\n",
        "\n",
        "model_1.compile(loss=tf.keras.losses.categorical_crossentropy\n",
        "                ,optimizer=tf.keras.optimizers.Adam()\n",
        "                ,metrics=[\"accuracy\"]\n",
        "                )\n",
        "\n",
        "model_1.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9PDRJRrl5ct",
        "outputId": "68cc087e-a7b0-4225-8385-3e0bc463efed"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of embedding tokens: (None, 55, 128)\n",
            "Model: \"conv1d\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization (TextVec  (None, 55)               0         \n",
            " torization)                                                     \n",
            "                                                                 \n",
            " model_0_embedding (Embeddin  (None, 55, 128)          8299648   \n",
            " g)                                                              \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, 55, 64)            41024     \n",
            "                                                                 \n",
            " global_average_pooling1d_3   (None, 64)               0         \n",
            " (GlobalAveragePooling1D)                                        \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 5)                 325       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,340,997\n",
            "Trainable params: 8,340,997\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we fit the model, let us create a function to push save our model experiments so that we can visualize it later on a tensorboard"
      ],
      "metadata": {
        "id": "DZyq7psT13nA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_DIR=\"model_log\""
      ],
      "metadata": {
        "id": "QguQfG_L3zCh"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_1 = model_1.fit(train_dataset\n",
        "                        ,steps_per_epoch=int(0.1 * len(train_dataset))\n",
        "                        ,validation_data=val_dataset\n",
        "                        ,validation_steps=int(0.1*len(val_dataset))\n",
        "                        ,epochs=5\n",
        "                        ,callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR,experiment_name=\"model_1_con1D\")]\n",
        "                        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8zQit4Z3Brb",
        "outputId": "525dc58c-3886-4fdd-9171-2159e6955407"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "562/562 [==============================] - 4s 7ms/step - loss: 0.3348 - accuracy: 0.8892 - val_loss: 0.6056 - val_accuracy: 0.7975\n",
            "Epoch 2/5\n",
            "562/562 [==============================] - 4s 7ms/step - loss: 0.3811 - accuracy: 0.8722 - val_loss: 0.5994 - val_accuracy: 0.7906\n",
            "Epoch 3/5\n",
            "562/562 [==============================] - 4s 6ms/step - loss: 0.3751 - accuracy: 0.8732 - val_loss: 0.6117 - val_accuracy: 0.7826\n",
            "Epoch 4/5\n",
            "562/562 [==============================] - 4s 7ms/step - loss: 0.3735 - accuracy: 0.8730 - val_loss: 0.6204 - val_accuracy: 0.7866\n",
            "Epoch 5/5\n",
            "562/562 [==============================] - 5s 10ms/step - loss: 0.3842 - accuracy: 0.8710 - val_loss: 0.6079 - val_accuracy: 0.7919\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1.evaluate(val_dataset)"
      ],
      "metadata": {
        "id": "2G5K1zPbGAOZ",
        "outputId": "243b2573-f253-4cad-ac67-07386ff47bb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "945/945 [==============================] - 6s 6ms/step - loss: 0.6139 - accuracy: 0.7905\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.613908588886261, 0.7905467748641968]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_pred_probs = model_1.predict(val_dataset)\n",
        "model_1_pred_probs"
      ],
      "metadata": {
        "id": "-Nh9ZzscKIdv",
        "outputId": "5aef5d14-31f7-4b04-f6ac-0a6ce9c6fb70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "945/945 [==============================] - 3s 3ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6.50308192e-01, 1.03300311e-01, 2.78954636e-02, 3.16770673e-02,\n",
              "        1.86818942e-01],\n",
              "       [4.44698811e-01, 3.24537635e-01, 3.78293246e-02, 1.42939821e-01,\n",
              "        4.99944538e-02],\n",
              "       [6.27319217e-02, 5.15101186e-04, 1.83221520e-04, 9.36565578e-01,\n",
              "        4.29708052e-06],\n",
              "       ...,\n",
              "       [3.12207158e-06, 6.66928245e-05, 1.31489139e-03, 7.11696111e-06,\n",
              "        9.98608172e-01],\n",
              "       [1.01191126e-01, 3.57440352e-01, 1.47733673e-01, 2.01272722e-02,\n",
              "        3.73507589e-01],\n",
              "       [1.82238594e-02, 9.73136485e-01, 8.07589106e-03, 3.73565854e-05,\n",
              "        5.26360120e-04]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_preds = tf.argmax(model_1_pred_probs,axis=1)\n",
        "model_1_preds"
      ],
      "metadata": {
        "id": "R9QJ1bhzKPyl",
        "outputId": "9d18bf09-eb3f-4d9b-a9fb-7137c6e317d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(30212,), dtype=int64, numpy=array([0, 0, 3, ..., 4, 4, 1])>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_1_results = calculate_results(y_true=val_labels_encoded\n",
        "                                    ,y_pred=model_1_preds)\n",
        "model_1_results"
      ],
      "metadata": {
        "id": "G0FIANfbKdLC",
        "outputId": "43dcade1-d4ef-413e-9059-4b00e89ed9a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 79.05468025949953,\n",
              " 'precision': 0.7893911188617301,\n",
              " 'recall': 0.7905468025949953,\n",
              " 'f1score': 0.7889346714244675}"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 2: Feature extraction with pretrained token embeddings\n",
        "As we move towards replicating the model architecture in [Neural Networks for Joint Sentence Classification in Medical Paper Abstracts](https://arxiv.org/pdf/1612.05251.pdf), it mentions they have used a [pretrained GLoVe embedding](https://nlp.stanford.edu/projects/glove/) as a way to initialise their token embeddings.\n",
        "\n",
        "For our learning purpose , we will use whatever is available from Tensorflow hub. [GLoVe embedding](https://nlp.stanford.edu/projects/glove/) is not available in Tensorflow hub, hence we will use [pretrained Universal Sentence Embedding from Tensorflow hub](https://tfhub.dev/google/universal-sentence-encoder/4)\n",
        "\n",
        "_This type of model is called **transfer learning**, or more specifically **feature extraction transfer learning**. In other words, taking the patterns a model has learned elsewhere and applying it to our own problem_"
      ],
      "metadata": {
        "id": "YZ2i4cWLNCwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "tf_hub_embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
        "                                        ,trainable=False\n",
        "                                        ,name=\"universal_sentence_encoder\")\n"
      ],
      "metadata": {
        "id": "BBi4ZMDiKubS"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_training_sentence= random.choice(train_sentences)\n",
        "print(f\" Training sample sentence: {random_training_sentence}\")\n",
        "use_embedding_sentence = tf_hub_embedding_layer([random_training_sentence])\n",
        "print(f\" Shape of the embedding vector generated use Universal Sentence Encoder : {use_embedding_sentence.shape}\")\n",
        "print(f\" Sentence embedding :{use_embedding_sentence[0][:30]}\")\n",
        "print(f\" Length of sentence embedding: {len(use_embedding_sentence[0])}\")"
      ],
      "metadata": {
        "id": "f0oB8tEXPtZT",
        "outputId": "bc8076ee-749a-4e2d-9d34-337ac4ebc834",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Training sample sentence: the observed burden of disease on hrqol was substantial compared to general us population norms , particularly in patients experiencing mixed episodes .\n",
            " Shape of the embedding vector generated use Universal Sentence Encoder : (1, 512)\n",
            " Sentence embedding :[-0.03586817  0.06236901  0.05411987 -0.01921409  0.04776343 -0.05777011\n",
            " -0.00871227  0.00082124  0.04027747 -0.01534815  0.096218    0.07446761\n",
            " -0.02940534  0.03609757  0.00127322 -0.02022783 -0.09728239 -0.02116657\n",
            " -0.0364463  -0.04369736 -0.05842407 -0.02718638  0.00630635  0.01993157\n",
            "  0.00415252  0.08268427  0.04450927  0.04528374  0.00986655  0.00790043]\n",
            " Length of sentence embedding: 512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  Building and fitting an NLP feature extraction model from Tensorflow hub"
      ],
      "metadata": {
        "id": "7ANoI1DSR_Px"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define feature extraction model from Tensorflow hub\n",
        "inputs = layers.Input(shape=[],dtype=tf.string)\n",
        "pretrained_embedding = tf_hub_embedding_layer(inputs)\n",
        "x = layers.Dense(128,activation=\"relu\")(pretrained_embedding)\n",
        "outputs = layers.Dense(num_classes,activation=\"softmax\")(x)\n",
        "model_2 = tf.keras.Model(inputs,outputs,name=\"feature_extraction_model_2\")\n",
        "\n",
        "model_2.compile(loss=tf.keras.losses.categorical_crossentropy\n",
        "                ,optimizer=tf.keras.optimizers.Adam()\n",
        "                ,metrics=[\"accuracy\"]\n",
        "                )\n",
        "\n",
        "model_2.summary()"
      ],
      "metadata": {
        "id": "VveUb9hHRuwQ",
        "outputId": "1b5738f2-3dc0-489d-e316-14fd9637f293",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"feature_extraction_model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None,)]                 0         \n",
            "                                                                 \n",
            " universal_sentence_encoder   (None, 512)              256797824 \n",
            " (KerasLayer)                                                    \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 128)               65664     \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 5)                 645       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 256,864,133\n",
            "Trainable params: 66,309\n",
            "Non-trainable params: 256,797,824\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history_2 = model_2.fit(train_dataset\n",
        "                        ,steps_per_epoch=(0.1 * len(train_dataset))\n",
        "                        ,validation_data=val_dataset\n",
        "                        ,validation_steps=(0.1 * len(val_dataset))\n",
        "                        ,epochs=5\n",
        "                        ,callbacks=[create_tensorboard_callback(dir_name=SAVE_DIR,experiment_name=\"feature_extractor_model_2\")]\n",
        "                        )"
      ],
      "metadata": {
        "id": "1rkdmcboTZ32",
        "outputId": "72dbe5cf-087d-42a8-c1fc-081cdb67d7cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "562/562 [==============================] - 14s 17ms/step - loss: 0.9193 - accuracy: 0.6490 - val_loss: 0.7993 - val_accuracy: 0.6878\n",
            "Epoch 2/5\n",
            "562/562 [==============================] - 9s 17ms/step - loss: 0.7650 - accuracy: 0.7027 - val_loss: 0.7546 - val_accuracy: 0.7046\n",
            "Epoch 3/5\n",
            "562/562 [==============================] - 8s 14ms/step - loss: 0.7477 - accuracy: 0.7123 - val_loss: 0.7385 - val_accuracy: 0.7148\n",
            "Epoch 4/5\n",
            "562/562 [==============================] - 9s 16ms/step - loss: 0.7149 - accuracy: 0.7259 - val_loss: 0.7089 - val_accuracy: 0.7273\n",
            "Epoch 5/5\n",
            "562/562 [==============================] - 9s 16ms/step - loss: 0.7212 - accuracy: 0.7225 - val_loss: 0.6902 - val_accuracy: 0.7296\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_2.evaluate(val_dataset)"
      ],
      "metadata": {
        "id": "nDBYxmaZT-Su",
        "outputId": "20bf1054-ad87-401a-8944-d371a8207eaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "945/945 [==============================] - 20s 22ms/step - loss: 0.6949 - accuracy: 0.7358\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6949051022529602, 0.735833466053009]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_2_pred_probs = model_2.predict(val_dataset)\n",
        "model_2_pred_probs"
      ],
      "metadata": {
        "id": "0O-oQUW7UQaw",
        "outputId": "9e426838-1dc0-479e-dcaf-dba443a1a0ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "945/945 [==============================] - 20s 21ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6.7330045e-01, 2.1049178e-01, 2.5145605e-03, 1.0646486e-01,\n",
              "        7.2283447e-03],\n",
              "       [4.3928412e-01, 4.6612373e-01, 5.2460879e-03, 8.7222479e-02,\n",
              "        2.1236546e-03],\n",
              "       [3.9072150e-01, 6.6590957e-02, 4.9541760e-02, 4.5547026e-01,\n",
              "        3.7675407e-02],\n",
              "       ...,\n",
              "       [2.0723410e-03, 2.6192572e-03, 8.9816190e-02, 4.8787362e-04,\n",
              "        9.0500432e-01],\n",
              "       [2.6994564e-03, 2.2442846e-02, 4.9094763e-01, 1.0812400e-03,\n",
              "        4.8282886e-01],\n",
              "       [2.2132280e-01, 3.3815470e-01, 3.8499328e-01, 2.8068647e-03,\n",
              "        5.2722234e-02]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_2_preds = tf.argmax(model_2_pred_probs,axis=1)\n",
        "model_2_preds"
      ],
      "metadata": {
        "id": "FXoz9AOXUWzG",
        "outputId": "e16623a3-1407-4120-a88f-1baa22dcfe14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(30212,), dtype=int64, numpy=array([0, 1, 3, ..., 4, 2, 2])>"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_2_results = calculate_results(y_true=val_labels_encoded\n",
        "                                    ,y_pred=model_2_preds)\n",
        "model_2_results"
      ],
      "metadata": {
        "id": "fgs_-7PUUdts",
        "outputId": "154dbb9c-bde0-4c56-a786-7d20284554f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 73.5833443664769,\n",
              " 'precision': 0.7331175270942084,\n",
              " 'recall': 0.735833443664769,\n",
              " 'f1score': 0.7302635554790833}"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 3: Conv1D with character embeddings\n",
        "\n",
        " The [Neural Networks for Joint Sentence Classification in Medical Paper Abstracts](https://arxiv.org/pdf/1612.05251.pdf) papers suggests use of a hybrid approach of character and token embeddings.\n",
        "\n",
        " **Character embedding** are generated using sequences split into characters whereas **token embedding** is created on sequences split into tokens"
      ],
      "metadata": {
        "id": "x__H-2iQU-Mw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A function to convert sentences into sequence of characters\n",
        "def split_chars(text):\n",
        "  return \" \".join(list(text))\n",
        "\n",
        "split_chars(random_training_sentence)"
      ],
      "metadata": {
        "id": "jVP30c7YU82v",
        "outputId": "d9ac93ec-91f8-4a30-eb66-3aecb9a067d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'t h e   o b s e r v e d   b u r d e n   o f   d i s e a s e   o n   h r q o l   w a s   s u b s t a n t i a l   c o m p a r e d   t o   g e n e r a l   u s   p o p u l a t i o n   n o r m s   ,   p a r t i c u l a r l y   i n   p a t i e n t s   e x p e r i e n c i n g   m i x e d   e p i s o d e s   .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will have to create character-level datasets by splitting our sequence dataset into characters"
      ],
      "metadata": {
        "id": "ReSsk2uKWpga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_chars = [split_chars(sentence) for sentence in train_sentences]\n",
        "val_chars = [split_chars(sentence) for sentence in val_sentences]\n",
        "test_chars = [split_chars(sentence) for sentence in test_sentences]"
      ],
      "metadata": {
        "id": "-6Hx7YxTU13h"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Average chars in training dataset\n",
        "char_size = [len(sentence) for sentence in train_sentences]\n",
        "avg = sum(char_size)/len(train_sentences)\n",
        "avg"
      ],
      "metadata": {
        "id": "5NY0nJKIXGsI",
        "outputId": "b858b912-932a-49f9-cc80-9a50cd67a17a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "149.3662574983337"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the average number of characters that 95% for sentences have in our training dataset\n",
        "output_seq_char_length = int(np.percentile(char_size,95))\n",
        "output_seq_char_length"
      ],
      "metadata": {
        "id": "yF59q1N6X1Np",
        "outputId": "6b9a3a73-4eaa-4710-97d6-8b2b2794024a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "290"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The whole idea of finding out the `output_seq_char_length` is to use the same value for `output_sequence_length` parameter while initializing `TextVectorization`\n",
        "\n",
        "We will have to set `max_token` parameter value to a value such that it includes\n",
        "1. Number of characters in english alphabet.\n",
        "2. Number of digits\n",
        "3. Number of punctuations\n",
        "4. Out Of Vocabulary\n",
        "5. Space character"
      ],
      "metadata": {
        "id": "HnGOl1JcY0Cc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "alphabet = string.ascii_lowercase + string.digits + string.punctuation\n",
        "alphabet"
      ],
      "metadata": {
        "id": "LXdgHWBbYpNY",
        "outputId": "3ae3ce2f-85ee-4f08-af9e-ade75277aeb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'abcdefghijklmnopqrstuvwxyz0123456789!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CHARS_TOKEN = len(alphabet) +2\n",
        "NUM_CHARS_TOKEN"
      ],
      "metadata": {
        "id": "3iB2EWuLaK_n",
        "outputId": "4d357352-8bd2-48da-8731-45c51db7e5e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "70"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bvfic8iQanKX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}