{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4j/ydtI0JGaY70LvXZ4KJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bikash119/learn_tensorflow/blob/main/nlp_tf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Character RNN"
      ],
      "metadata": {
        "id": "fJSRDkd4hY15"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the Training Dataset"
      ],
      "metadata": {
        "id": "39oxR1Bshp2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "filepath= tf.keras.utils.get_file(\"shakespeare_txt\",\n",
        "                                  origin=\"https://homl.info/shakespeare\")\n",
        "with open(filepath) as fp:\n",
        "  shakespeare_tex= fp.read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSfY-hdahuXV",
        "outputId": "2c9acb06-e68b-4c62-884a-112a3a68eebb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://homl.info/shakespeare\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(shakespeare_tex[:80])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHaypQ-0iKRc",
        "outputId": "d13104f2-568c-4f83-ecc6-d3dd6faa6800"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform Text Vectorization"
      ],
      "metadata": {
        "id": "VQGtDeVbidUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform character text vectorization\n",
        "text_vec_layer= tf.keras.layers.TextVectorization(split=\"character\",\n",
        "                                                  standardize=\"lower\")\n",
        "text_vec_layer.adapt([shakespeare_tex])"
      ],
      "metadata": {
        "id": "Zo-g_gUiimTf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sentence=\" I am here to learn nlp\"\n",
        "print(len(sample_sentence))\n",
        "## Vectorize the sample sentence\n",
        "text_vec_layer([sample_sentence])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yua72JxejsLR",
        "outputId": "c2046840-b72b-4eb8-ef3a-4f674b4daee5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 23), dtype=int64, numpy=\n",
              "array([[ 2,  7,  2,  6, 16,  2,  8,  3, 10,  3,  2,  4,  5,  2, 13,  3,\n",
              "         6, 10, 11,  2, 11, 13, 24]])>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens= text_vec_layer([shakespeare_tex])[0]\n",
        "# Print some text from original text\n",
        "print(f\" Original Text : {shakespeare_tex[:80]} \")\n",
        "# Print the vectorized text\n",
        "print(f\" Vectorized tokens: {tokens[:80]}\")\n",
        "print(len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cLpUhChkq77",
        "outputId": "0ca646ea-6dd3-474d-9b32-ccea6d2e3aab"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Original Text : First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak. \n",
            " Vectorized tokens: [21  7 10  9  4  2 20  7  4  7 37  3 11 25 12 23  3 21  5 10  3  2 18  3\n",
            "  2 24 10  5 20  3  3 14  2  6 11 17  2 21 15 10  4  8  3 10 19  2  8  3\n",
            "  6 10  2 16  3  2  9 24  3  6 26 28 12 12  6 13 13 25 12  9 24  3  6 26\n",
            " 19  2  9 24  3  6 26 28]\n",
            "1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Unique token characters in vocabulary\n",
        "chars_in_vocab= text_vec_layer.get_vocabulary()\n",
        "top_5_chars= chars_in_vocab[:5]\n",
        "bottom_5_chars= chars_in_vocab[-5:]\n",
        "print(f\" Number of characters in vocab :{text_vec_layer.vocabulary_size()}\")\n",
        "print(f\" top 5 common tokens: {top_5_chars}\")\n",
        "print(f\" bottom 5 common tokens: {bottom_5_chars}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDgnTKtVo0bE",
        "outputId": "5f6b7fd7-f751-408f-d746-ac2e873f67c7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Number of characters in vocab :41\n",
            " top 5 common tokens: ['', '[UNK]', ' ', 'e', 't']\n",
            " bottom 5 common tokens: ['x', 'z', '3', '&', '$']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokens -= 2 # Remove the 0 (pad) and 1(UNK) token\n",
        "n_tokens = text_vec_layer.vocabulary_size() - 2\n",
        "dataset_size= len(tokens)\n",
        "print(f\" dataset size : {len(tokens)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIwIfx36qHDl",
        "outputId": "a59a8c82-53e7-4de3-e061-f367df1bbb74"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " dataset size : 1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perpare Dataset"
      ],
      "metadata": {
        "id": "Sv3sRJuT2TSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n",
        "  ds= tf.data.Dataset.from_tensor_slices(sequence)\n",
        "  ds= ds.window(length+1, shift=1, drop_remainder=True)\n",
        "  ds= ds.flat_map(lambda window_ds: window_ds.batch(length+1))\n",
        "  if shuffle:\n",
        "    ds= ds.shuffle(buffer_size=100_000,seed=seed)\n",
        "  ds= ds.batch(batch_size)\n",
        "  return ds.map(lambda window: (window[:, :-1], window[:,1:])).prefetch(1)\n"
      ],
      "metadata": {
        "id": "isjnUF8s2Y7z"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "length= 100\n",
        "tf.random.set_seed(42)\n",
        "train_set= to_dataset(tokens[:100_000],length=length\n",
        "                      , shuffle=True,seed=42)\n",
        "valid_set= to_dataset(tokens[1_000_000:1_060_000],length=length)\n",
        "test_set= to_dataset(tokens[1_600_000:],length=length)"
      ],
      "metadata": {
        "id": "RFnjyqir3ZcP"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating an Embedding using an Embedding Layer"
      ],
      "metadata": {
        "id": "Gi3Rbr790nSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "embedding_layer = tf.keras.layers.Embedding(input_dim=n_tokens\n",
        "                                            ,output_dim=16\n",
        "                                            ,input_length=length)\n",
        "embedding_layer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yKSKD8b1MeZ",
        "outputId": "4d514401-7013-40aa-8bc4-54e6b9b3ee52"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.layers.core.embedding.Embedding at 0x7b78a1cc9b40>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sentenc= \" I am here to learn NLP\"\n",
        "print(f\" Sample Sentenc : {sample_sentence}\")\n",
        "tokens_of_sample_sentence= text_vec_layer(sample_sentence)\n",
        "print(f\" length of tokens from TextVectorizer for sample sentence: {len(tokens_of_sample_sentence)}\")\n",
        "embeddings_of_sample_sentence= embedding_layer(tokens_of_sample_sentence)\n",
        "print(f\" Shape of Embeddings from embeding layer for sample sentence: {embeddings_of_sample_sentence.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duhtKwwT4I-e",
        "outputId": "c091f060-b140-4f41-b265-f2d8b61ff908"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Sample Sentenc :  I am here to learn nlp\n",
            " length of tokens from TextVectorizer for sample sentence: 23\n",
            " Shape of Embeddings from embeding layer for sample sentence: (23, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building and Training the Char-RNN model\n"
      ],
      "metadata": {
        "id": "ivQ_Nl1D45gs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "char_model= tf.keras.Sequential([\n",
        "    embedding_layer\n",
        "    ,tf.keras.layers.GRU(128, return_sequence=True)\n",
        "    ,tf.keras.layers.Dense(n_tokens,activation=\"softmax\")\n",
        "])\n",
        "\n",
        "char_model.compile(oss=tf.keras.losses.sparse_categorical_crossentropy\n",
        "                   ,optimizer=tf.keras.optimizers.Nadam()\n",
        "                   ,metrics=['accuracy'])\n",
        "model_ckpt= tf.keras.callbacks.ModelCheckPoint(\"my_shakespeare_model\"\n",
        "                                               ,monitor=\"val_accuracy\"\n",
        "                                               ,save_best_only=True)\n",
        "history= char_model.fit(train_set\n",
        "                        ,validation_data=valid_set\n",
        "                        ,epochs=2\n",
        "                        ,callbacks=[model_ckpt])"
      ],
      "metadata": {
        "id": "zxmDvFQn6j3X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}